{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black; padding: 10px;\">\n",
    "<h1>Kaggle petfinder adoption prediction</h1>\n",
    "\n",
    "<h2>Work undertaken</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Links</h3>\n",
    "<a href= \"https://github.com/ecepep/kaggle_petfinder\">Github's repo</a> <br>\n",
    "<a href= \"https://www.kaggle.com/c/petfinder-adoption-prediction\">Kaggle's competition</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>This jupyter summaries my work for the  kaggle competition \"petfinder: adoption prediction\". Exhaustive uncleaned source code of trials is to be found in the github's repository.</p>\n",
    "<p>(My work station: 8gb ram, i5 3337u 2*1.8ghz, no gpu)</p>\n",
    "<p>I am using python 3.6 through conda. The environement will require (keras (>2.2.3), sci-kit, numpy, scipy, tensorflow, xgboost)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary</h3><br>\n",
    "<a href=\"#Preprocessing\">1) Preprocessing</a><br>\n",
    "<a href=\"#Metrics\">2) Metrics' definition</a><br>\n",
    "<a href=\"#RandomForestClassifier_bench\">3) RandomForestClassifier - bench</a><br>\n",
    "<a href=\"#CustomNNCategorical\">4) CustomNNCategorical</a><br>\n",
    "<a href=\"#Other_preprocessed_features \">5) Other preprocessed features </a><br>\n",
    "<a href=\"#xgb_features_selection\">6) Xgb features selection</a><br>\n",
    "<a href=\"#Conclusion\">7) Conclusion </a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Preprocessing\">Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Retrieve data from CSV.</li>\n",
    "<li>Set to NA unmeaningful values.</li>\n",
    "<li>Merge the preprocessed metadatas (explanations about metadata preprocessing is a later topic).</li>\n",
    "<li>Define some transformers to be used in sci-kit Pipeline.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readBaseCSV(path, shuffle = True,\n",
    "                  isNa = {\"Health\" : 0, \"MaturitySize\" : 0, \"FurLength\" : 0, \"Gender\" : 3,\n",
    "                                \"Vaccinated\" : 3, \"Dewormed\" : 3, \"Sterilized\" : 3}):\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # replace \"undefined|not sure\" values to na\n",
    "    for i in isNa.keys():\n",
    "        toNa = (df[i] == isNa[i]).sum()\n",
    "        df[i] = df[i].replace(isNa[i], np.nan)\n",
    "  \n",
    "    if shuffle:  df = df.take(np.random.permutation(df.shape[0]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_train_test(path_to_all = \"../all/\"):\n",
    "    trainPath = path_to_all + \"/train.csv\"\n",
    "    testPath = path_to_all + \"/test/test.csv\"\n",
    "\n",
    "    # read csv and set some to na\n",
    "    train = readBaseCSV(trainPath, shuffle = True)\n",
    "    test = readBaseCSV(testPath, shuffle = True)\n",
    "    return train, test\n",
    "\n",
    "def get_train_test_meta_img(path_to_all = \"../all/\"):\n",
    "    meta_dir = path_to_all + \"/preprocessed/metadata_label/\"\n",
    "    img_dir = path_to_all + \"/preprocessed/transfered_img/\"\n",
    "    \n",
    "    # read train, test csvs, set unknown to NA and shuffle\n",
    "    train, test = get_train_test(path_to_all, silent = silent)\n",
    "    # add features preprocessed from metadata dir :see preprocessed/prep_metadata\n",
    "    train, test = merge_metadata(meta_dir, train, test)\n",
    "    # add features from the preprocessed img (through a frozen cnn)\n",
    "    train, test = merge_img_fcnn(img_dir, train, test)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> transformers: </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import sparse\n",
    "import re\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Select columns in dataframe according to their name. \n",
    "    '''\n",
    "    def __init__(self, attribute_names, dtype=None, ravel = False, regex = False):\n",
    "        '''\n",
    "        \n",
    "        :param attribute_names:\n",
    "        :param dtype: output type\n",
    "        :param ravel: convert output shape of (n, 1) to (n,)\n",
    "        '''\n",
    "        self.attribute_names = attribute_names\n",
    "        self.dtype = dtype\n",
    "        self.ravel = ravel\n",
    "        self.regex = regex\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.regex:\n",
    "            X_selected = X.filter(regex=self.attribute_names)\n",
    "        else:\n",
    "            X_selected = X[self.attribute_names]\n",
    "            \n",
    "        if self.dtype:\n",
    "            return X_selected.astype(self.dtype).values\n",
    "        \n",
    "        if self.ravel & (X_selected.shape[1] == 1):\n",
    "            return X_selected.values.ravel()\n",
    "        \n",
    "        return X_selected.values\n",
    "    \n",
    "class StringConcat(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    concat several string features to a single string \n",
    "    '''\n",
    "    def __init__(self, sep = \" \"):\n",
    "        '''\n",
    "        :param sep: separator\n",
    "        '''\n",
    "        self.sep = sep\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _concat(self, s):\n",
    "        return self.sep.join(s)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        remove_sep = lambda s: re.sub(r\"[^A-Za-z0-9]\", \"\", s)\n",
    "        X = np.vectorize(remove_sep)(X)\n",
    "        return np.apply_along_axis(self._concat, axis = 1, arr = X)\n",
    "\n",
    "class FnanToStr(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    replace float nan to \"\"\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def on_array(x_str):\n",
    "        '''\n",
    "        replace float nan to \"\"\n",
    "        :param x: 1D array with bool sel\n",
    "        '''\n",
    "        # print(list(a[v_is_str(a)])[1] == float(\"nan\")) # false, why??\n",
    "        is_str = lambda x: not type(x) is str\n",
    "        v_is_str = np.vectorize(is_str)\n",
    "        x_str[v_is_str(x_str)] = \"\"\n",
    "        return x_str\n",
    "\n",
    "    def transform(self, X):\n",
    "#         train.Description.fillna(\"none\")\n",
    "        if len(X.shape) == 1:        \n",
    "            return FnanToStr.on_array(X)\n",
    "        else:\n",
    "            for i in range(0, X.shape[1]):\n",
    "                X[:, i] = FnanToStr.on_array(X[:, i])\n",
    "            return X\n",
    "        \n",
    "class Formater(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Base for transformer to change format of X after transform in Pipe\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X \n",
    "    \n",
    "class Multiplier(Formater):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def transform(self, X):\n",
    "        return X * self.factor\n",
    "    \n",
    "class DimPrinter(Formater):\n",
    "    '''\n",
    "    To be applied just before classification. Print the number of dimension fed to clf.\n",
    "    '''\n",
    "    def __init__(self, extra = None):\n",
    "        self.extra = extra\n",
    "        self.printOnce = True\n",
    "    def transform(self, X):\n",
    "        if self.printOnce:\n",
    "            if self.extra: print(self.extra)\n",
    "            print(\"X dim print\", X.shape)\n",
    "            self.printOnce = False\n",
    "        return X\n",
    "    \n",
    "class ToSparse(Formater):\n",
    "    def transform(self, X):\n",
    "        return sparse.csr_matrix(X)\n",
    "    \n",
    "class Reshape(Formater):\n",
    "    def __init__(self, shape = -1):\n",
    "        '''\n",
    "        :param shape: -1  to reduce dim of 1\n",
    "        '''\n",
    "        self.shape = shape\n",
    "    def transform(self, X):\n",
    "        return X.reshape(self.shape)\n",
    "    \n",
    "class Ravel(Formater):\n",
    "    '''\n",
    "    necessary before tf-idf or AttributeError: 'numpy.ndarray' object has no attribute 'apply'\n",
    "    '''\n",
    "    def _ravel(self, X):\n",
    "        if len(X.shape)==1: return X\n",
    "        elif X.shape[1] == 1: return X.ravel()\n",
    "        else: raise \"unexpected Ravel()\"\n",
    "    def transform(self, X):\n",
    "        return self._ravel(X) \n",
    "    \n",
    "class AsType(Formater):\n",
    "    def __init__(self, astype):\n",
    "        self.astype = astype\n",
    "    def transform(self, X):\n",
    "        return X.astype(self.astype)\n",
    "\n",
    "class PipeLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "     Wraps sklearnâ€™s LabelEncoder, but encodes unseen data in your test \n",
    "     set as a default factor-level. Accept more than one column.\n",
    "    \n",
    "    @note equivalent: class skutil.preprocessed.SafeLabelEncoder\n",
    "    \n",
    "    :warning hardcoded\n",
    "    \"\"\"\n",
    "    def __init__(self, silent = True):\n",
    "        self.values = list()\n",
    "        super().__init__()\n",
    "        self.labels = []\n",
    "        self.silent = silent\n",
    "        \n",
    "    def fit(self, Xt, y=None):\n",
    "        for i in range(0, Xt.shape[1]):\n",
    "            self.labels.append(np.unique(Xt[:,i]))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, Xt):            \n",
    "        assert Xt.shape[1] == len(self.labels)\n",
    "        for i in range(0, Xt.shape[1]):\n",
    "            # Test set might have values yet unknown to the classifiers\n",
    "            unknown = np.setdiff1d(np.unique(Xt[:,i]), self.labels[i], assume_unique=True)\n",
    "            if (len(unknown) > 0) and (not self.silent) : print(len(unknown), \"unknown labels found.\")\n",
    "            \n",
    "            uValues = np.append(self.labels[i], unknown)\n",
    "            # all unknown values will take the same extra label\n",
    "            futurLabel = list(range(0, self.labels[i].size)) + [self.labels[i].size]*len(unknown)\n",
    "            mapping = dict(zip(uValues, futurLabel))\n",
    "            \n",
    "            f = lambda i, mapping: mapping[i] \n",
    "            Xt[:,i] = np.vectorize(f)(Xt[:,i], mapping)\n",
    "        return Xt\n",
    "    \n",
    "class PipeOneHotEncoder(PipeLabelEncoder):\n",
    "    \"\"\"\n",
    "    Extend PipeLabelEncoder to one hot encoding.\n",
    "    :warning not using sparse matrix, but np 2D\n",
    "    :warning hardcoded\n",
    "    \"\"\"\n",
    "    def __init__(self, silent = True):\n",
    "        PipeLabelEncoder.__init__(self, silent = silent)\n",
    "    \n",
    "    def fit(self, Xt, y=None):\n",
    "        PipeLabelEncoder.fit(self, Xt, y)\n",
    "        self.nums_label = [len(self.labels[i]) for i in range(0,len(self.labels))] # number of label\n",
    "        return self\n",
    "        \n",
    "    def transform(self, Xt):            \n",
    "        assert Xt.shape[1] == len(self.labels)\n",
    "        Xt = PipeLabelEncoder.transform(self, Xt)\n",
    "        XtOH = np.zeros((Xt.shape[0], sum(self.nums_label)+1)) # Xt in one hot encode notation\n",
    "        cumsum_len = np.cumsum([0] + self.nums_label[:-1])\n",
    "        for ji in range(0, Xt.shape[1]):\n",
    "            for i in range(0, Xt.shape[0]):\n",
    "                if not Xt[i,ji] > self.nums_label[ji]: # 'unknown' label is still coded as [0,0,0,0,0,0] \n",
    "                    XtOH[i, cumsum_len[ji]+Xt[i,ji]] = 1\n",
    "        \n",
    "        return XtOH\n",
    "\n",
    "class ColorBreedOH(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Concat all colors or breeds together in one single one hot encoding \n",
    "    to divide their dim by respectively 3 and ~~2\n",
    "    '''\n",
    "    def __init__(self, weights, silent = True):\n",
    "        self.onehot = PipeOneHotEncoder(silent = silent)\n",
    "        self.weights = weights\n",
    "                \n",
    "    def fit(self, Xt, y=None):     \n",
    "        '''\n",
    "        learn the level for Breed0, Breed1, Breed2 together\n",
    "        '''\n",
    "        self.onehot.fit(np.reshape(Xt, (np.product(Xt.shape), 1)))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, Xt):\n",
    "        Xt_transform = None\n",
    "        self.weights = [1]* Xt.shape[1] if self.weights is None else self.weights\n",
    "        \n",
    "        for j in range(0, Xt.shape[1]):\n",
    "            oh_col = self.onehot.transform(np.reshape(Xt[:,j], (Xt[:,j].shape[0], 1)))\n",
    "            if Xt_transform is None:\n",
    "                Xt_transform = oh_col * self.weights[j]\n",
    "            else:\n",
    "                Xt_transform = Xt_transform + oh_col * self.weights[j]\n",
    "        \n",
    "        return Xt_transform\n",
    "                \n",
    "class InferNA(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    infer na to mean of value (even for unordered value because they are all binary)\n",
    "    '''\n",
    "    def __init__(self, attribute_names, method=\"mean\"):\n",
    "        '''\n",
    "        :param attribute_names: feature for which NAs will be infered\n",
    "        :param method:\n",
    "        '''       \n",
    "        self.method = method \n",
    "        self.attribute_names = attribute_names\n",
    "        self.replacement =  dict()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        assert self.method == \"mean\"\n",
    "        for i in self.attribute_names:\n",
    "            self.replacement[i] = X.loc[:,i].mean(skipna=True) \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X =  X.copy() \n",
    "        for i in self.attribute_names:\n",
    "            X.loc[:,i] = X.loc[:,i].replace(np.nan, self.replacement[i], inplace = False)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Metrics\">Metrics' definition</h3>\n",
    "<p>The kaggle's competition defined the metric to be a quadratic cohen's kappa. Sci-kit implements it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "def quadratic_cohen_kappa(y_true, y_pred):\n",
    "        return metrics.cohen_kappa_score(y_true, y_pred, weights = \"quadratic\")\n",
    "    \n",
    "qwk_scorer = make_scorer(quadratic_cohen_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"RandomForestClassifier_bench\">RandomForestClassifier - bench</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>base for the pipeline</b> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) separate features by properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### csv features\n",
    "# feat used in num pipe\n",
    "numeric_features = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt']\n",
    "ordinal_features = ['MaturitySize', 'FurLength', 'Health']\n",
    "binary_features = ['Gender', 'Vaccinated', 'Dewormed', 'Sterilized']\n",
    "\n",
    "# unordered nominal features aka factors\n",
    "breed = ['Breed1', 'Breed2']\n",
    "color = ['Color1', 'Color2', 'Color3']\n",
    "low_dim_only = breed + color + ['State']\n",
    "nominal_features = low_dim_only + ['RescuerID']\n",
    "\n",
    "# text features (name is never used)\n",
    "text_features = [\"Name\", \"Description\"]\n",
    "\n",
    "# \n",
    "not_a_feat = [\"AdoptionSpeed\", \"PetID\"]\n",
    "\n",
    "# feat set with NAs at get_train_test, NAs to infer as mean of col\n",
    "feat_with_nas = ordinal_features + binary_features\n",
    "\n",
    "### preprocessed \n",
    "# metadata preprocesssed features\n",
    "expected_len = 10\n",
    "meta_labels = [\"label\" + str(i) for i in range(0, expected_len)]\n",
    "meta_label_scores = [\"label_score\" + str(i) for i in range(0, expected_len)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) define subpipeline which describe further dynamic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Select data that can be considered as numeric. Standardize.\n",
    "num_pipe = Pipeline([\n",
    "            ('sel_num', DataFrameSelector(numeric_features + binary_features + ordinal_features, dtype = 'float32')),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "# Label Encode nominal features\n",
    "nom_pipe_label_encode = Pipeline([\n",
    "            ('sel_nom', DataFrameSelector(nominal_features)),\n",
    "            ('encoder', PipeLabelEncoder(silent=True)),\n",
    "            ('astype', AsType(astype = \"float32\"))\n",
    "        ])\n",
    "\n",
    "# Tf-idf the description\n",
    "des_pipe = Pipeline([\n",
    "            ('sel_num', DataFrameSelector([\"Description\"], ravel = True)),\n",
    "            ('rm_nan', FnanToStr()),\n",
    "            (\"ravel\", Ravel()),\n",
    "            ('tfid_vect', TfidfVectorizer(max_df= 0.743, min_df=0.036, ngram_range=(1,4),\\\n",
    "strip_accents='ascii', analyzer= \"word\", stop_words='english', norm = \"l1\", use_idf = True))\n",
    "        ])\n",
    "\n",
    "# to sparse array\n",
    "num_pipe_sparse = Pipeline([\n",
    "            ('num_pipe', num_pipe),\n",
    "            (\"sparse\", ToSparse())\n",
    "        ])\n",
    "nom_pipe_label_encode_sparse = Pipeline([\n",
    "            ('nom_pipe_label_encode', nom_pipe_label_encode),\n",
    "            (\"sparse\", ToSparse())\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>define RandomForestClassifier pipeline.</b>  \n",
    "<ul>\n",
    "<li>Infer the value set as NA while preprocessing, to the mean.</li>\n",
    "<li>Combine tf-idf's description, label encoded nominal and standardized numerical (aka numerical+binary+ordinal) features.</li>\n",
    "<li>Classify.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "\n",
    "# add desccription features \n",
    "pipe_rdf_des = Pipeline([\n",
    "    ('infer_na_mean', InferNA(feat_with_nas, method = \"mean\")), \n",
    "    ('u_prep', FeatureUnion([\n",
    "        ('num_pipe', num_pipe_sparse),\n",
    "        ('nom_pipe_label_encode', nom_pipe_label_encode_sparse),\n",
    "        ('des_pipe', des_pipe)\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators = 200)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Run pipeline</b>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.369\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from sklearn.model_selection._search import GridSearchCV\n",
    "\n",
    "path_to_all = \"./all\"\n",
    "train, test = get_train_test(path_to_all) # test is for submission only\n",
    "n_cpu = cpu_count()-1\n",
    "grid_search = GridSearchCV(pipe_rdf_des, param_grid = {}, scoring = qwk_scorer,\n",
    "                           cv=3,n_jobs=n_cpu,return_train_score = True) \n",
    "grid_search.fit(train, train[\"AdoptionSpeed\"])\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_) # ~=0.365\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison between the training score (>0.99) and test score (~=0.365) already hint for some issues with overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"CustomNNCategorical\">CustomNNCategorical</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>@note: With only roughly 15 000 rows, I did not expect a deep neural network to really score better than a bagging tree method but for the sake of learning, I tried to achieve comparable results with both. Indeed, I was looking for some opportunities to experiment the knowledge I acquired with <a href=\"https://www.coursera.org/specializations/deep-learning\">Andrew NG's moocs</a>. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CustomNNCategorical classifier</b>\n",
    "<p>The class below implements a classifier for sci-kit learn pipeline. Internally, it uses a custom deep neural network made of several Dense layer of keras' library.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class offers the possibility to:\n",
    "<ul>\n",
    "<li>Define the number of layers and their respectiv widths.</li>\n",
    "<li>Add dropout and/regularization to each layer.</li>\n",
    "<li>Reduce dynamically the learning rate.</li>\n",
    "<li>Use cohen kappa as a custom metric. (for early stopping)</li>\n",
    "<li>Early stop.</li>\n",
    "<li>\"Adam\" optimizer is kept as default.</li>\n",
    "<li>The loss used is \"categorical crossentropy\". @note: it does not take for account that our output is ordered and metrics is a quadratic kappa.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class CustomNNBase(BaseEstimator, ClassifierMixin):  \n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "    def __init__(self, epoch, loss, optimizer, metrics, batch_size):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        self.model = None\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def __compile(self, input_shape, output_shape):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "#         self.predict(X)\n",
    "#         res = model.evaluate(x_test, y_test, verbose=0)\n",
    "        raise \"no defined score\"\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f22c4f908adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_nn_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomNNBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNN_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordinal_categorical_crossentropy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlossOCCQuadratic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossOCC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/eclipse-workspace/kaggle_petfinder/NN_loss/ordinal_categorical_crossentropy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m '''\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from classification.custom_nn_base import CustomNNBase\n",
    "from NN_loss.ordinal_categorical_crossentropy import lossOCCQuadratic, lossOCC\n",
    "\n",
    "from sklearn.preprocessing.label import LabelEncoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input, InputLayer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.nn import relu, softmax\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import sleep\n",
    "from keras import regularizers\n",
    "from sklearn import metrics as metrics\n",
    "from sklearn.model_selection._split import train_test_split\n",
    "from classification.Cohen_kappa_logger import Cohen_kappa_logger\n",
    "\n",
    "\n",
    "class CustomNNCategorical(CustomNNBase):  \n",
    "    \"\"\"\n",
    "    Base for custom sk classifier implementing NN using keras\n",
    "    \n",
    "    implement an MLP for classification with custom metric cohen kappa, no custom loss for now @todo\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden=[200, 100, 50, 20], dropout=[0.1, 0.1], reg=[0.05,0.05], h_act=[relu],\n",
    "                       epoch=500, batch_size=32,  cbEarly=\"metric\", loss=\"categorical_crossentropy\", \n",
    "                       optimizer='adam', metrics=['cohen_kappa'], kappa_weights=\"quadratic\",\n",
    "                       validation = 0.2, smooth_cb = True\n",
    "                ):\n",
    "        '''        \n",
    "        :param hidden:\n",
    "        :param dropout:  dropout[0] is assigned to input then hidden\n",
    "        :param reg: ularization \n",
    "        :param h_act: hidden_actication\n",
    "        :param epoch:\n",
    "        :param batch_size:\n",
    "        :param cbEarly: \"metric\" or an EarlyStopping instance\n",
    "        :param loss:\n",
    "        :param optimizer:\n",
    "        :param metrics: \"Accuracy\" or 'cohen_kappa'\n",
    "        :param kappa_weights: compatible with sk(ex:\"quadratic\", None) ignored if metrics != 'cohen_kappa'\n",
    "        :param smooth_cb: if True EarlyStopping use val_cohen_kappa smoothed (left avg window 3),\n",
    "         only with val_cohen_kappa\n",
    "        \n",
    "        :note restore_best_weights requires keras 2.2.3\n",
    "        \n",
    "        '''\n",
    "        assert loss in [\"categorical_crossentropy\", lossOCC, lossOCCQuadratic]\n",
    "        CustomNNBase.__init__(self, epoch, loss, optimizer, metrics, batch_size)\n",
    "        assert (len(hidden) > 0) & (len(hidden)+1 >= len(dropout)) & \\\n",
    "            (len(hidden) >= len(reg)) & (len(hidden) >= len(h_act))\n",
    "            \n",
    "        self.hidden = hidden\n",
    "        self.dropout = dropout\n",
    "        self.reg = reg\n",
    "        self.h_act = h_act\n",
    "        self.validation = validation\n",
    "        \n",
    "        self.final_activation = softmax\n",
    "        \n",
    "        self.cbEarly = cbEarly\n",
    "        self.smooth_cb = smooth_cb\n",
    "        \n",
    "        \n",
    "        self.cbReduceLR = ReduceLROnPlateau(\n",
    "            monitor='loss', factor=0.8, patience=3,\n",
    "            verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "        \n",
    "        self.kappa_weights = kappa_weights\n",
    "        if len(self.metrics) > 1 : raise \"TODO\"\n",
    "    \n",
    "        \n",
    "    def __compile(self, input_shape, output_shape):\n",
    "        ter = lambda x, i: None if len(x) <= i else x[i]\n",
    "        reg = [regularizers.l2(i) for i in self.reg] #@TODO ALSO USE L1 FOR BETTER FEATURE SELECTION\n",
    "        h_act = self.h_act * round(len(self.hidden) / len(self.h_act))\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(InputLayer(input_shape=(input_shape,)))\n",
    "        if not ter(self.dropout, 0) is None: self.model.add(Dropout(ter(self.dropout, 0)))\n",
    "        \n",
    "        for i in range(0, len(self.hidden)):\n",
    "            self.model.add(Dense(self.hidden[i], activation=h_act[i],\n",
    "                                kernel_regularizer=ter(reg, i), bias_regularizer=ter(reg, i)))                       \n",
    "            if not ter(self.dropout, i+1) is None: self.model.add(Dropout(ter(self.dropout, i+1))) # first for input\n",
    "        \n",
    "        self.model.add(Dense(output_shape, activation=self.final_activation))\n",
    "        \n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "              loss=self.loss,\n",
    "              metrics=self.metrics)\n",
    "\n",
    "    def __category_to_output(self, y):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        y = self.label_encoder.fit_transform(y)\n",
    "        target = to_categorical(y, num_classes=np.unique(y).size)\n",
    "        return target\n",
    "    \n",
    "    def __output_to_category(self, output):\n",
    "        pred = [np.argmax(i) for i in output]\n",
    "        pred = self.label_encoder.inverse_transform(pred)\n",
    "        return pred\n",
    "\n",
    "    def cohen_kappa_metric_keras(self, y_true, y_pred):\n",
    "        '''\n",
    "        Do not work as a metric because kappa is not linear and keras make a weighted avg of batches score\n",
    "        :deprecated @see Cohen_kappa_logger\n",
    "        '''\n",
    "        raise \"deprecated @see Cohen_kappa_logger\"\n",
    "        return tf.py_func(self.cohen_kappa_score, [y_true, y_pred], tf.float32)       \n",
    "    \n",
    "    def cohen_kappa_score(self, y_true, y_pred): \n",
    "        raise \"deprecated @see Cohen_kappa_logger\"\n",
    "        y_pred = self.__output_to_category(y_pred)\n",
    "        y_true = self.__output_to_category(y_true)\n",
    "        \n",
    "        score = metrics.cohen_kappa_score(y_true, y_pred, weights=self.kappa_weights)\n",
    "        return score.astype(np.float32)\n",
    "\n",
    "    def break_on_epoch_n(self, threshold, sec = 60):\n",
    "        self.n_epoch = len(self.history.history[\"loss\"])\n",
    "        if self.n_epoch > threshold:\n",
    "            sleep(sec)  # cool down\n",
    "    \n",
    "    def _fit_val(self, X, output):\n",
    "        # @todo clean below\n",
    "        if type(self.validation) is float:\n",
    "            self.history = self.model.fit(\n",
    "                X, output,\n",
    "                validation_split = self.validation,\n",
    "                epochs=self.epoch,\n",
    "                batch_size=self.batch_size,\n",
    "                callbacks=self.callback_list,\n",
    "                verbose=0)\n",
    "            \n",
    "        elif type(self.validation) is tuple:\n",
    "            assert self.validation[0].shape[1] == X.shape[1], \"X_validation must be transformed with prep first\"\n",
    "            self.validation = (self.validation[0], self.__category_to_output(self.validation[1]))\n",
    "            self.history = self.model.fit(\n",
    "                X, output,\n",
    "                validation_data = self.validation,\n",
    "                epochs=self.epoch,\n",
    "                batch_size=self.batch_size,\n",
    "                callbacks=self.callback_list,\n",
    "                verbose=0)\n",
    "            \n",
    "        elif self.validation is None:\n",
    "            self.history = self.model.fit(\n",
    "                X, output,\n",
    "                epochs=self.epoch,\n",
    "                batch_size=self.batch_size,\n",
    "                callbacks=self.callback_list,\n",
    "                verbose=0)\n",
    "        else: raise \"unknown validation type\"\n",
    "    \n",
    "    def _kappa_disambiguation(self, X, output):\n",
    "        '''\n",
    "        :param X:\n",
    "        :param output:\n",
    "        '''\n",
    "        self.metric_plot = None\n",
    "        self.patience = 20 #for cbEarly is enoughfrom observation @todo in init\n",
    "                    \n",
    "        if self.metrics[0] == \"accuracy\":\n",
    "            self.metric_plot = \"acc\"\n",
    "            raise \"min_delta must be redefined according to val_acc\"\n",
    "            if self.use_smooth_cb:\n",
    "                raise 'not available for acc self.use_smooth_cb'\n",
    "            if self.cbEarly == \"metric\":\n",
    "                self.cbEarly = EarlyStopping(\n",
    "                    monitor= 'val_acc' if self.validation else \"acc\", min_delta=0.0001, \n",
    "                    patience=self.patience, verbose=0, mode='auto')\n",
    "            self.kappa_logger = None\n",
    "            \n",
    "        elif self.metrics[0] == 'cohen_kappa':\n",
    "            self.metrics = None # 'cohen_kappa_metric' cannot be supported @see explication in Cohen_kappa_logger\n",
    "            self.metric_plot = 'cohen_kappa'\n",
    "            if self.cbEarly == \"metric\":\n",
    "                if self.validation:\n",
    "                    monitor = \"val_cohen_kappa_smoothed\" if self.smooth_cb else \"val_cohen_kappa\" \n",
    "                else:\n",
    "                    if not self.smooth_cb:\n",
    "                        monitor = \"cohen_kappa\"\n",
    "                    else: raise \"No cohen_kappa_smoothed\"\n",
    "                print(\"monitor\", monitor)\n",
    "                self.cbEarly = EarlyStopping(\n",
    "                                    monitor = monitor if self.validation else \"cohen_kappa\",\n",
    "                                    min_delta=0.00000001, patience=self.patience, # a large patience is necessary!\n",
    "                                    verbose=0, mode='max', restore_best_weights=True)\n",
    "            \n",
    "            if type(self.validation) is float:\n",
    "                X, X_val, output, y_val = train_test_split(X, output, test_size = self.validation)\n",
    "            elif type(self.validation) is tuple:\n",
    "                assert self.validation[0].shape[1] == X.shape[1], \"X_validation must be transformed with prep first\"\n",
    "                X_val = self.validation[0]\n",
    "                y_val = self.__category_to_output(self.validation[1])\n",
    "            elif not self.validation is None: raise \"unknown validation type\"\n",
    "            \n",
    "#             self.validation = None # can slightly reduce computation but need val_loss for callback LRReduceOnPlateau \n",
    "\n",
    "            self.kappa_logger = Cohen_kappa_logger(\n",
    "                 output_to_category=self.__output_to_category,\n",
    "                 X_train = X, y_train = output, \n",
    "                 X_val = X_val, y_val = y_val, \n",
    "                 kappa_weights = self.kappa_weights)\n",
    "            \n",
    "        else: \n",
    "            print(self.metrics[0])\n",
    "            raise \"not implemented\"\n",
    "        return X, output\n",
    "            \n",
    "    def fit(self, X, y=None):\n",
    "        '''\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :param cbEarly: Parameter for early stopping\n",
    "        '''\n",
    "        output = self.__category_to_output(y)\n",
    "        \n",
    "        X, output = self._kappa_disambiguation(X, output)\n",
    "        \n",
    "        output_shape = output.shape[1]\n",
    "        input_shape = X.shape[1]\n",
    "        self.__compile(input_shape, output_shape)\n",
    "      \n",
    "        self.callback_list = []\n",
    "        for cb in [self.kappa_logger, self.cbReduceLR, self.cbEarly]:\n",
    "            if cb: self.callback_list.append(cb)\n",
    "        \n",
    "        self._fit_val(X, output)\n",
    "        \n",
    "        self.break_on_epoch_n(50)          \n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"history\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"Call fit first.\")\n",
    "        \n",
    "        preds = self.model.predict(X)\n",
    "        preds = self.__output_to_category(preds)\n",
    "        return preds\n",
    "\n",
    "    def plot_history(self, plotname=\"NN\", saving_file=None):\n",
    "        '''\n",
    "        :param plotname:\n",
    "        :param saving_file: filename where to save plots\n",
    "        :return plt , to avoid carbage collection and closing of the windows\n",
    "        '''\n",
    "        history = self.history\n",
    "        plot = (saving_file is None)\n",
    "#         print(\"History acc\", history.history['acc'])\n",
    "#         print(\"History loss\", history.history['loss'])\n",
    "#         print(\"History lr\", history.history['lr'])\n",
    "#         print(\"Acc train (last)\", history.history['acc'][-5:-1])\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if plot: plt.ion()\n",
    "        if plot: plt.show()\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.grid(True)\n",
    "        plt.title(plotname)\n",
    "#         print(\"possible plot\", history.history.keys())\n",
    "        if self.metric_plot in history.history.keys():\n",
    "            plt.subplot(221)\n",
    "            plt.plot(history.history[self.metric_plot])\n",
    "            plt.ylabel(self.metric_plot + \"  \")\n",
    "            if plot: plt.draw()\n",
    "        \n",
    "        if \"val_\" + self.metric_plot in history.history.keys():\n",
    "            plt.subplot(222)\n",
    "    #         print(\"possible plot\", history.history.keys())\n",
    "            plt.plot(history.history[\"val_\" + self.metric_plot])\n",
    "            plt.ylabel(\"val_\" + self.metric_plot + \"  \")\n",
    "            if plot: plt.draw()\n",
    "    \n",
    "            if False: \n",
    "                print(\"self.patience last epochs\")\n",
    "                print(history.history[\"val_\" + self.metric_plot][-(self.patience+1):])            \n",
    "        \n",
    "        plt.subplot(223)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.ylabel('\"loss\" ' + \"  \" + plotname)\n",
    "        if plot: plt.draw()\n",
    "        \n",
    "        plt.subplot(224)\n",
    "        if \"val_cohen_kappa_smoothed\" in history.history.keys():\n",
    "            plt.plot(history.history['val_cohen_kappa_smoothed'])\n",
    "            plt.ylabel(\"val_cohen_kappa_smoothed\")\n",
    "        else:\n",
    "            plt.plot(history.history['lr'])\n",
    "            plt.ylabel('\"lr\"' + \"  \" + plotname)\n",
    "        if plot: plt.draw()\n",
    "        if plot: plt.pause(1)\n",
    "\n",
    "        if saving_file:\n",
    "            fig.savefig(saving_file)\n",
    "            plt = None # send to carbage\n",
    "                      \n",
    "        return plt\n",
    "       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CohenKappaLoger</b>\n",
    "<p>This neural network tend to overfit as well even with dropout and/or regularization. To help with this issue and to reduce computation time(#no gpu), the class CohenKappaLoger implements a custom (quadratic) cohen kappa metric to be used along side keras EarlyStopping. </p><p>First was considered a real custom metrics rather than a callback (@see @deprecated cohen_kappa_metric_keras). However, Keras calculate the metrics by averaging batches' value. Unfortunately, this works with accuracy but not with cohen's kappa which is a non-linear function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.classification import cohen_kappa_score as sk_cohen_kappa_score\n",
    "from keras.callbacks import Callback\n",
    "from numpy import mean\n",
    "\n",
    "class Cohen_kappa_logger(Callback):\n",
    "    '''\n",
    "    Add to the logs \"val_cohen_kappa\" and \"cohen_kappa\" at each epoch's end to record cohen's kappa score.\n",
    "    Works fine along with EarlyStopping. val_cohen_kappa is avg smoothed\n",
    "    '''\n",
    "    def __init__(self, output_to_category=None,\n",
    "                 X_train = None, y_train = None, \n",
    "                 X_val = None, y_val = None, \n",
    "                 kappa_weights = \"quadratic\",\n",
    "                 smooth_window = 5):\n",
    "        '''\n",
    "        \n",
    "        :param output_to_category:\n",
    "        :param X_train:\n",
    "        :param y_train:\n",
    "        :param X_val:\n",
    "        :param y_val:\n",
    "        :param kappa_weights:\n",
    "        :param smooth_window: help avoiding overfitting with earlier early stopping\n",
    "        '''\n",
    "        self.output_to_category = output_to_category or (lambda x: x)\n",
    "        self.X_val = X_val\n",
    "        self.X_train = X_train\n",
    "        self.y_val = y_val\n",
    "        self.y_train = y_train\n",
    "        self.kappa_weights = kappa_weights\n",
    "        self.smooth_window = smooth_window\n",
    "        if X_val is None: raise 'implement none X_val COhen kappa logger'\n",
    "        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred_train = self.model.predict(self.X_train)\n",
    "        pred_val = self.model.predict(self.X_val)\n",
    "        \n",
    "        score_train = self.cohen_kappa(pred_train, self.y_train)\n",
    "        score_val = self.cohen_kappa(pred_val, self.y_val)\n",
    "        \n",
    "        if not \"val_cohen_kappa\" in self.model.history.history.keys(): # not defined on first run\n",
    "            smoothed_val_score = score_val # mean(self.model.history.history[\"val_cohen_kappa\"][-(self.smooth_window+1):])\n",
    "        else: \n",
    "            last_score = self.model.history.history[\"val_cohen_kappa\"][-(self.smooth_window-1):]\n",
    "            last_score.append(score_val)\n",
    "            smoothed_val_score = mean(last_score)\n",
    "            \n",
    "        logs[\"cohen_kappa\"]= np.float64(score_train)\n",
    "        logs[\"val_cohen_kappa\"]= np.float64(score_val)\n",
    "        logs[\"val_cohen_kappa_smoothed\"]= np.float64(smoothed_val_score)\n",
    "        return\n",
    "\n",
    "    def cohen_kappa(self, y_true, y_pred): \n",
    "        y_pred = self.output_to_category(y_pred)\n",
    "        y_true = self.output_to_category(y_true)\n",
    "           \n",
    "        return sk_cohen_kappa_score(y_true, y_pred, weights=self.kappa_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CustomNNOrdered</b>\n",
    "<p>This class tries to give some importance to the ordering of the ordinal predicted value by using another representation, loss function and activation. It performs similar scores as the original network. </p>\n",
    "<i>A more sophisticated method might be more useful here (Weighted kappa loss function for multi-class classification of ordinal data in deep learning, Jordi de la Torrea, Domenec Puiga, Aida Valls || A simple squared-error reformulation for ordinal classification, Christopher Beckham & Christopher Pal).</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classification.custom_nn_categorical import CustomNNCategorical\n",
    "\n",
    "from tensorflow.nn import sigmoid\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "class CustomNNordered(CustomNNCategorical):\n",
    "    '''\n",
    "    implement an mlp classifier for ordered categorical\n",
    "    Ys represented as:\n",
    "        0: [0, 0, 0, 0, 1],1: [0, 0, 0, 1, 1],2: [0, 0, 1, 1, 1]...\n",
    "    loss: \"binary_crossentropy\"\n",
    "    final activation: sigmoid\n",
    "        \n",
    "    simplistic solution to infer importance of ordering to the NN.\n",
    "    '''\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        self.loss = None\n",
    "#         assert loss\n",
    "#         assert cbEarly (if None ==> define else warning acc)\n",
    "        super(CustomNNordered, self).__init__(*args, **kwargs)\n",
    "\n",
    "        if not self.loss is None: warnings.warn(\"loss will be set as 'binary_crossentropy'\")\n",
    "        self.loss = \"binary_crossentropy\"\n",
    "\n",
    "        # acc is bad, cb early with lower delta\n",
    "        if \"cbEarly\" in kwargs:\n",
    "            warnings.warn(\"Accuracy from keras is wrong for CustomNNordered. Choose the monitor wisely\")\n",
    "            \n",
    "        self.final_activation = sigmoid\n",
    "    \n",
    "    def __category_to_output(self, y):\n",
    "        '''\n",
    "        :param y: array of target ordered categories. Categories must be number from 0 to n-1 (ordered)\n",
    "        '''\n",
    "        n_cat = y.unique().size\n",
    "        assert set(y.unique()) == set(range(0,n_cat)), \\\n",
    "            \"rewrite more exhaustive fun (toOrderedCategorical)\"     \n",
    "        target = [([0]*(c-i) + [1]*i) for i, c in zip(y, [n_cat]*len(y))]\n",
    "        return np.array(target)\n",
    "    \n",
    "    def __output_to_category(self, output):\n",
    "        pred = output.round().astype(int)\n",
    "        pred =  [i.sum() for i in pred]\n",
    "        return pred\n",
    "    \n",
    "    def plot_history(self, plotname=\"NN\", saving_file=None):\n",
    "        warnings.warn(\"Accuracy from keras is wrong for CustomNNordered. Choose the monitor wisely\")\n",
    "        CustomNNCategorical.plot_history(self, plotname, saving_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>pipeline for mlp</b><br>\n",
    "<p>I am using one hot encoding with the labeled data. Level for colors and breeds is high. To reduce the number of dimension, the o-h transformers for those features encode all colors and breeds on the same vector (almost no info loss, dim / 2 or 3).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of nominal_features\n",
    "colors_oh = Pipeline([\n",
    "            ('sel_nom', DataFrameSelector(color)),\n",
    "            ('encoder', ColorBreedOH([1,0.6,0.3], silent=True)),\n",
    "            ('astype', AsType(astype = \"float64\"))\n",
    "        ])\n",
    "breeds_oh = Pipeline([\n",
    "            ('sel_nom', DataFrameSelector(breed)),\n",
    "            ('encoder', ColorBreedOH([1,1], silent=True)),\n",
    "            ('astype', AsType(astype = \"float64\"))\n",
    "        ])\n",
    "state_oh = Pipeline([\n",
    "            ('sel_nom', DataFrameSelector([\"State\"])),\n",
    "            ('encoder', PipeOneHotEncoder(silent=True)),\n",
    "            ('astype', AsType(astype = \"float64\")),\n",
    "        ])\n",
    "# Rescuer_ID has to many level to be relevant in one hot encoding. Is ignored.\n",
    "low_dim_nom_pipe_oh= Pipeline([\n",
    "            (\"ulowdim\", FeatureUnion([\n",
    "                (\"colors\", colors_oh),\n",
    "                (\"breed\", breeds_oh),\n",
    "                (\"state\", state_oh)\n",
    "                ]))\n",
    "        ])\n",
    "\n",
    "# NN classifier\n",
    "basicNN = CustomNNCategorical(hidden = [400, 200, 100, 50, 20], dropout = [0.1,0.5,0.5,0.5,0.5], reg = [],\n",
    "        h_act = [relu], epoch = epoch, cbEarly = \"metric\",\n",
    "        metrics = ['cohen_kappa'])\n",
    "\n",
    "# NN classifier @see CustomNNordered\n",
    "orderedNN = CustomNNordered(\n",
    "        hidden = [400, 200, 100, 50, 20],\n",
    "        dropout = [0.1,0.5,0.5,0.5,0.5,0.5], reg = [],\n",
    "        h_act = [relu], epoch = epoch, cbEarly = \"metric\",\n",
    "        metrics = ['cohen_kappa'])\n",
    "\n",
    "# pipe for mlp, similar to pipe_rdf_des but with NN classifier and one hot encoded label\n",
    "pipe_mlp_oh_des = Pipeline([\n",
    "    ('infer_na_mean', InferNA(feat_with_nas, method = \"mean\")), \n",
    "    ('u_prep', FeatureUnion([\n",
    "        ('num_pipe', num_pipe_sparse),\n",
    "        (\"low_dim_nom_pipe_oh\", low_dim_nom_pipe_oh),\n",
    "        ('des_pipe', des_pipe),\n",
    "    ])),\n",
    "    ('clf', copy.deepcopy(basicNN))\n",
    "])\n",
    "\n",
    "def replace_step(pipe, step_name, new_step):\n",
    "    '''\n",
    "    :param pipe: original pipe\n",
    "    :param step_name: as string :ex \"clf\"\n",
    "    :param new_step: the usual tupple :ex (\"clf\", RandomForest())\n",
    "    :return a new pipe where the step as been replace\n",
    "    \n",
    "    :warning has to used pipe.steps and not pipe.named_steps\n",
    "    '''\n",
    "    for step in range(0, len(pipe.steps)):\n",
    "        if pipe.steps[step][0] ==  step_name:\n",
    "            new_pipe = copy.deepcopy(pipe)\n",
    "            new_pipe.steps[step] = new_step\n",
    "            return  new_pipe\n",
    "    raise Exception(\"step not found:\"+step_name)\n",
    "\n",
    "# pipe_mlp_oh_des with alternate NN@see CustomNNordered\n",
    "pipe_mlp_oh_des_orderedNN = replace_step(pipe_mlp_oh_des, \"clf\", ('clf', copy.deepcopy(orderedNN)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observations</b>\n",
    "<ul>\n",
    "<li>\n",
    "basicNN is the most relevant parameterization I found for this problem. The lack of computation's power did not allowed me to precisely optimize the parameterization.\n",
    "</li>\n",
    "<li>\n",
    "Dropout shows itself more efficient and general than the regularization which tend to bring issues in the propagation.\n",
    "</li>\n",
    "<li>\n",
    "A dropout of 0.5 has been chosen since it theoretically optimizes regularization but for the first layer (0.1).\n",
    "</li>\n",
    "<li>\n",
    "The precision of EarlyStopping is really important. A smoothing has been added to the evaluation of the model to garanty an early enough stopping.\n",
    "</li>\n",
    "<li>\n",
    "Empirically, there are some clear evidence that the dimension of the input has a strong impact on the score for both NN and rdf (higher dim -> ovrefit).<br>\n",
    "The base pipeline (label encoded) has 19+125 features. 125 corresponds to the features from the tf-idfed description. However, the description tend to increase the score by ~0.02/0.03 regardless of the model.\n",
    "Though the concatenation of breeds and colors in one hot encoding reduce the dimension from more than 100, it curiously performs similar with NN (a dummy test with rdf shows its correct implementation).\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3 id=\"Other_preprocessed_features\">Other preprocessed features</h3>\n",
    "<p>In addition to the initial dataset, petfinder provides json's relative to the pet pictures generated from google API. The above mentionned pictures are available as well.</p>\n",
    "<p>Two possibilities to make of it extra features have been implemented.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>google API's Json</b>\n",
    "<p>From the jsons is extracted the \"labelAnnotations\" and their respective scores. Those values are saved as pkl for later merging in the whole dataset at the preprocessing step. @see @file /preprocessed/metadata.py</p>\n",
    "<p>For simplicity and to avoid the curse of dimension. Each picture labels are concatenated to a string which is then processed through TfidfVectorizer.</p>\n",
    "<p>A more sophisticated preprocessing notably involving the scores could probably yield better results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringConcat(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    concat several string features to a single string \n",
    "    '''\n",
    "    def __init__(self, sep = \" \"):\n",
    "        '''\n",
    "        :param sep: separator\n",
    "        '''\n",
    "        self.sep = sep\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _concat(self, s):\n",
    "        return self.sep.join(s)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        remove_sep = lambda s: re.sub(r\"[^A-Za-z0-9]\", \"\", s)\n",
    "        X = np.vectorize(remove_sep)(X)\n",
    "        return np.apply_along_axis(self._concat, axis = 1, arr = X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_label_simple_concat_pipe = Pipeline([\n",
    "            ('sel_label', DataFrameSelector(meta_labels)),\n",
    "            ('rm_nan', FnanToStr()),\n",
    "            ('concat_labels', StringConcat()),\n",
    "            (\"ravel\", Ravel()),\n",
    "            ('tfid_vect', TfidfVectorizer(max_df= 0.743, min_df=0.036, ngram_range=(1,5),\\\n",
    "strip_accents='ascii', analyzer= \"word\", stop_words = None, norm = \"l1\", use_idf = True)),\n",
    "])\n",
    "\n",
    "pipe_rdf_meta_only = pipe_rdf_extra_dim = Pipeline([\n",
    "    ('meta_label_simple_concat_pipe', meta_label_simple_concat_pipe),\n",
    "    ('clf', RandomForestClassifier(n_estimators = 200)),\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Observation </i>\n",
    "<ul>\n",
    "<li>\n",
    "<p>The labels' annotations processed by google API are probably unrelevant. It mostly retrieve labels as \"dogs\", \"dogs breed\", \"carnivoran\" (hence the tf-idf) or the name of the breed which is a repetition to an information that already exist in the more reliable  dataset.</p>\n",
    "</li>\n",
    "<li>\n",
    "Though pipe_rdf_meta_only gives a non null prediction (~~0.1), combined with the original pipeline, it performs the same.\n",
    "</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>pet pictures</b>\n",
    "<p>@note: Again, this has been implemented for the fun (of using keras) since it is very unlikely to bring valuable result.</p><p> It is aimed at bringing a (slightly) deeper understanding of the pic than the metadata from googles API.</p>\n",
    "<p>It works as such:</p>\n",
    "<ul>\n",
    "<li>For each pet a single picture is retrieved and resized. @see @file /preprocessed/image_transfer.py</li>\n",
    "<li>A frozen pretrained (on \"imagenet\") VGG16 prived of its last layer predicts the pic.</li>\n",
    "<li>Predictions are saved to .pkl to be merged later at preprocessing.</li>\n",
    "<li>A pca is applied as a desperate solution to avoid adding 1000 features.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras import backend\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.misc.pilutil import imresize\n",
    "import sys\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "\n",
    "class FrozenCnn():\n",
    "    '''\n",
    "    classdocs\n",
    "    Predict img from a frozen pretrained cnn \n",
    "    '''\n",
    "    def __init__(self, cnn = VGG16, inputshape = (224,224), n_pop=1):\n",
    "        '''\n",
    "        Constructor\n",
    "        \n",
    "        :param cnn: cnn to be loaded with imagenet\n",
    "        :param inputshape: input shape of the CNN, img will be resized to it \n",
    "        :param n_pop: number of popped layer\n",
    "        '''\n",
    "        self.cnn = cnn\n",
    "        self.inputshape = inputshape\n",
    "        if self.cnn == VGG16 and self.inputshape != (224,224): raise \"input shape does not match keras VGG16\"\n",
    "        self.n_pop = n_pop\n",
    "        \n",
    "    def __compile(self, input_shape, output_shape):\n",
    "        pass\n",
    " \n",
    "    def fit(self, X = None, y=None):\n",
    "        if not X is None: print(\"frozen does not fit to X! (pretrained on imagenet)\")\n",
    "        \n",
    "        vgg_conv = self.cnn(weights ='imagenet', include_top=True)\n",
    "        \n",
    "        for layer in vgg_conv.layers:\n",
    "            layer.trainable = False\n",
    "#             print(layer)\n",
    "#             print(\"layer out\", layer.output)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(vgg_conv)\n",
    "        \n",
    "        for i in range(0, self.n_pop):\n",
    "            model.layers.pop() # Get rid of the classification layer  \n",
    "            while type(model.layers[-1]) is Dropout:\n",
    "                model.layers.pop() # Get rid of the dropout layer\n",
    "        \n",
    "        print(\"output:\", model.layers[-1].output)\n",
    "            \n",
    "        self.model = model   \n",
    "        return self\n",
    " \n",
    "    def predict(self, X, y=None):\n",
    "        '''\n",
    "        use plt.show to show pictures where resize deform with h or w twice bigger\n",
    "        :param X:\n",
    "        :param y:\n",
    "        '''\n",
    "        # load pretrained cnn from keras\n",
    "        \n",
    "        image_resized = []\n",
    "        for img in X:\n",
    "            image_resized.append(imresize(img, self.inputshape))\n",
    "            if img.shape[0] / img.shape[1] > 2 or img.shape[0] / img.shape[1] < 0.5: \n",
    "                print(\"warning on im resize shape,\", img.shape)\n",
    "#                 fig=plt.figure()\n",
    "#                 fig.add_subplot(1,2,1)\n",
    "#                 plt.imshow(img)\n",
    "#                 fig.add_subplot(2,2,2)\n",
    "#                 plt.imshow(image_resized[-1])\n",
    "#                 plt.draw()\n",
    "\n",
    "        prediction = self.model.predict(np.array(image_resized))\n",
    "#         backend.clear_session() # (was) necessary to avoid memory leakage\n",
    "        return prediction\n",
    "           \n",
    " \n",
    "    def score(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    def __del__(self):\n",
    "        backend.clear_session() # could create trouble du to carbage collector being late???\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline with img's 1000 features, scaled\n",
    "pipe_img = Pipeline([\n",
    "    (\"sel_imgf\", DataFrameSelector(\"imgf_[0-9]+\", regex = True)), \n",
    "    (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "# with pca\n",
    "pipe_img_PCA = Pipeline([(\"sel_imgf\", pipe_img),\n",
    "                         (\"PCA\", PCA(n_components=30))])\n",
    "\n",
    "\n",
    "# with NN classifier\n",
    "pipe_mlp_img_only = pipe_rdf_extra_dim = Pipeline([\n",
    "    (\"pipe_img\", pipe_img),\n",
    "    ('clf', copy.deepcopy(basicNN)),\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Observation </i>\n",
    "<ul>\n",
    "<li>\n",
    "As expected, added to the feature union pipe_img_PCA does not bring any improvements (rather the opposite). However, I was satisfied to see that pipe_mlp_img_only manages better predictions than pipe_rdf_meta_only confirming that predicted images have more info than google's api metadata.\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"xgb_features_selection\">Xgb features selection</h3>\n",
    "<p>As already mentionned dimensionnality is a first concern here. As a more concrete attempt to improve the score, gradient boosted trees can be used to improve features' selection. For itself, it implements some efficient regularizations and it can even make bet on \"features importance\". </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_all = \"./all\" # path to dataset dir\n",
    "\n",
    "train, test = get_train_test_meta_img(path_to_all)\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, train[\"AdoptionSpeed\"], test_size = 0.3) \n",
    "\n",
    "pipe = pipe_rdf_des      \n",
    "\n",
    "#     https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "#    param of xgb were optimized to the pipe_rdf_des thanks to a search.\n",
    "pipe = replace_step(pipe, \"clf\", ('clf', copy.deepcopy(\n",
    "         xgb.XGBClassifier(max_depth=10, learning_rate=0.1, n_estimators=200, silent=True, objective='binary:logistic',\n",
    "                   booster='gbtree', n_jobs=1, nthread=None, gamma=0.1, min_child_weight=5, max_delta_step=0, \n",
    "                   subsample=0.8, colsample_bytree=0.7, colsample_bylevel=1, reg_alpha=0.65, reg_lambda=100,\n",
    "                    scale_pos_weight=1, base_score=0.5, random_state=0, \n",
    "                    seed=None, missing=None) \n",
    "        )) )\n",
    "xgb_clf = xgb.XGBClassifier(max_depth=10, learning_rate=0.1, n_estimators=200, silent=True, objective='binary:logistic',\n",
    "                   booster='gbtree', n_jobs=1, nthread=None, gamma=0.1, min_child_weight=5, max_delta_step=0, \n",
    "                   subsample=0.8, colsample_bytree=0.7, colsample_bylevel=1, reg_alpha=0.65, reg_lambda=100,\n",
    "                    scale_pos_weight=1, base_score=0.5, random_state=0, \n",
    "                    seed=None, missing=None) # return a pipeline with the same xgb as in pipe and without prep\n",
    "\n",
    "pipe.fit(x_train, y_train)    \n",
    "print(\"pipe.named_steps.clf.feature_importances_\",pipe.named_steps.clf.feature_importances_)\n",
    "\n",
    "prep_pipe = deepcopy(pipe)\n",
    "prep_pipe.steps = prep_pipe.steps[:-1]\n",
    "x_train_tf = prep_pipe.transform(x_train)\n",
    "x_test_tf = prep_pipe.transform(x_test)    \n",
    "\n",
    "# X_train_transf = pipe.transform(x_train)\n",
    "# X_test_transf = pipe.transform(x_test)\n",
    "threshold = [0, 0.0005, 0.001,0.002, 0.003]\n",
    "for th in threshold:\n",
    "    selection = SelectFromModel(pipe.named_steps.clf, threshold=th, prefit=True)\n",
    "    select_X_train = selection.transform(x_train_tf)\n",
    "    select_X_test = selection.transform(x_test_tf)\n",
    "    selection_model = xgb_clf\n",
    "    selection_model2 = RandomForestClassifier(n_estimators = 200)\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "    selection_model2.fit(select_X_train, y_train)\n",
    "    \n",
    "    select_pred_train = selection_model.predict(select_X_train)                                                \n",
    "    select_pred_test = selection_model.predict(select_X_test)      \n",
    "    select_pred_train2 = selection_model2.predict(select_X_train)                                                \n",
    "    select_pred_test2 = selection_model2.predict(select_X_test)                                                  \n",
    "                                                                         \n",
    "    select_score_train = quadratic_cohen_kappa(select_pred_train, y_train)                                         \n",
    "    select_score_test = quadratic_cohen_kappa(select_pred_test, y_test)  \n",
    "    select_score_train2 = quadratic_cohen_kappa(select_pred_train2, y_train)                                         \n",
    "    select_score_test2 = quadratic_cohen_kappa(select_pred_test2, y_test)  \n",
    "    \n",
    "    print(\"for threshold\", th)\n",
    "    print(\"select_score_train\", select_score_train)\n",
    "    print(\"select_score_test\", select_score_test) \n",
    "    print(\"select_score_train rdf\", select_score_train2)\n",
    "    print(\"select_score_test rdf\", select_score_test2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observations</b>\n",
    "<ul>\n",
    "<li>The features selection do not yield better results.</li>\n",
    "<li>Xgb performs as well as a rdf classifier.\n",
    "However, I believe that more precise parameterization could do slightly better.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Conclusion\">Conclusion</h3>\n",
    "<p>I took a lot of fun in all the attempts undertaken and it gave me the possibility to experiment with keras.</p>\n",
    "<p>A more realistic approach of the task would have resulted in higher score but I do not regret the things I learned since they were my true goal.</p>\n",
    "<p>Du to a lack of time, I cannot bring this project further. However, the todo list in the code is still very long. Some topics as preprocessing, regularization would disserve closer attention and some important steps were overlooked like looking at misclassified predictions or visualizing the fitted model...etc</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
